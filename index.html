<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Violence Detection</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Violence Detection From Videos</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/muratsehzade" target="_blank">Murat Åžehzade</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/hamza-etciba%C5%9F%C4%B1-445a31232/" target="_blank">Hamza Etcibasi</a>,</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/berke-bayraktar-0511ba195/" target="_blank">Berke Bayraktar</a>
                  </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <a href="https://avesis.hacettepe.edu.tr/aydin.kaya" target="_blank">Assoc. Prof. Dr. Aydin KAYA</a>
                    </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Hacettepe University<br>BBM479-480 Design Project</span>

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://github.com/b21827407/Design-Project" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="static/pdfs/report.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>End of Project Report</span>
                    </a>
                  </span>

                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->

                  <!-- Github link -->


                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">

        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div >
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size:large;">
            The exponential growth of video data, especially in security surveillance, presents a challenge in manual
            monitoring and a need for automation. In this context, our project focuses on detecting and classifying
            acts of violence from video data. Given the surge of interest in deep learning and computer vision for
            video monitoring tasks, we propose an approach grounded on both deep learning techniques and other
            proven methods for violence detection.</p>
          <p style="font-size:large;">We utilize a model based on Convolutional Neural Networks (CNN) and Long Short-Term Memory
              (LSTM) cells, specifically leveraging a pre-trained VGG19 as a spatial feature extractor. This model works
              by processing frames sequentially through the TimeWarp layer, which allows for the extraction of spatial
              features from each frame before the sequence is passed to the LSTM layer. The LSTM layer then learns
              the temporal relationships between frames. To optimize our model, we experimented with different
              hyperparameters such as learning rate, image size, number of frames, stride, and input types (RGB, flow,
              and a combination of RGB and flow). Results showed that an RGB input yielded 0.98 test accuracy being
              the highest, indicating its efficacy in our CNN-LSTM based model.</p>

          <p style="font-size:large;">
            The application and effectiveness of the proposed CNN-LSTM model, augmented by the incorporation of
            an ensemble of existing methodologies, have marked a significant stride in the field of automated
            violence detection and classification from video data. The efficacy of this model holds promising
            implications for the future of automated security surveillance systems. The expected impact of this study
            is considerable. The model demonstrates potential for deployment in real-world scenarios to enhance
            security surveillance operations, thereby providing a layer of automation that could transform how
            security monitoring is currently handled. This not only mitigates the risk of human error but also improves
            the efficiency of monitoring, particularly in areas with an extensive number of surveillance cameras.</p>
          <p style="font-size:large;">
            Going forward, there are several directions for future exploration. The developed model can be subjected
            to further fine-tuning and tested on a broader array of datasets for diversified violence scenarios. This
            could ensure a more versatile violence detection system. In addition, although the work focused on
            utilizing RGB input for optimal outcomes, future work can investigate other input modalities, such as
            infrared or thermal imaging, to improve model robustness in varied environments.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- <section class="section hero ">
  <div class="is-centered has-text-centered">
    <h2 class="title is-3">Methodology</h2>
    <div style="display: flex; justify-content: center; align-items: center;">
      <p class="subtitle has-text-centered" style=" max-width: 60%;">
        We utilize a model based on Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) cells, specifically leveraging a pre-trained VGG19 as a spatial feature extractor. This model works by processing frames sequentially through the TimeWarp layer, which allows for the extraction of spatial
      features from each frame before the sequence is passed to the LSTM layer. The LSTM layer then learns
      the temporal relationships between frames. To optimize our model, we experimented with different
      hyperparameters such as learning rate, image size, number of frames, stride, and input types (RGB, flow,
      and a combination of RGB and flow). Results showed that an RGB input yielded 0.98 test accuracy being
      the highest, indicating its efficacy in our CNN-LSTM based model.
      </p>
    </div>
    </div>

</section> -->
<!-- Image carousel -->



<section class="hero is-small ">
  <div class="is-centered has-text-centered" style="margin-top: 30px;">
    <h2 class="title is-3">Experiments</h2>

  </div>
  <h2 class="subtitle has-text-centered" style="margin-top: 30px;">
    We carried out our experiments starting with RGB inputs and chose many hyperparameters based on these experiments.
  </h2>
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
          <b>INPUT_TYPE = 'RGB' - LEARNING RATE EXPERIMENTS</b>
        </h2>
        <h2 class="subtitle has-text-centered">
          In the course of our experimentation, we conducted multiple trials using different learning rates within the range of 1e-3 to 5*1e-5. Based on our findings, we were able to determine that a learning rate of 1e-4 was most suitable for our particular model. This conclusion was reached after a thorough analysis of the results obtained from the various trials, and we believe it to be an optimal choice for achieving the desired outcomes.
        </h2>
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/1.png" alt="MY ALT TEXT"/>
        </div>

      </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
          <b>INPUT_TYPE = 'RGB' - IMAGE SIZE EXPERIMENTS</b>
        </h2>
        <h2 class="subtitle has-text-centered">
          Our experimental process involved testing various image sizes as a critical hyperparameter that significantly affects both training and test time. After conducting a series of experiments using image sizes of 100, 160, and 224, we found that larger image sizes were more effective in understanding video characteristics. Specifically, we observed that an image size of 224 yielded the highest accuracy and lowest test loss. However, it was also four times slower than the smallest image size of 100, while an image size of 160 was twice as slow as 100. Given these factors, we ultimately decided to adopt an image size of 100 as the default setting to prioritize overall speed in our processes.
        </h2>
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/2.png" alt="MY ALT TEXT"/>
        </div>

      </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
          <b>INPUT_TYPE = 'RGB' - NUMBER OF FRAMES EXPERIMENTS</b>
        </h2>
        <h2 class="subtitle has-text-centered">
          Throughout the course of our experimentation, we conducted numerous trials, utilizing varying numbers of frames ranging from 5 to 20. After analyzing the results, we were able to determine that 10 frames were the most suitable for our particular model. Our conclusion was reached after a thorough analysis of the data obtained from the various trials, and we strongly believe that this is the optimal choice for achieving the desired outcomes.
        </h2>
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/3.png" alt="MY ALT TEXT"/>
        </div>

     </div>
     <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
        <b>INPUT_TYPE = 'RGB' - STRIDE EXPERIMENTS</b>
      </h2>
      <h2 class="subtitle has-text-centered">
        Another parameter we put into test was stride to consider when extracting images from videos. We have tested 4 different parameters from stride=1 to stride=4. Based on results obtained, which can be seen on the graph below, we can see that stride of 3 performs best with a score of 0.94 by a small margin where as stride of 1 performs lowest with 0.92. This can be attributed to the fact that, algorithm is able to generalize better when looking at frames with stride of 3 instead of 1. Nonetheless, bigger stride doesntâ€™ always mean better results. As it can be seen on the graph, stride of 4 reduces score to 0.93. Although a marginal difference, it could be exacerbated when working on a larger dataset where it would make sense to try different stride values with more difference.
      </h2>
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/images/4.png" alt="MY ALT TEXT"/>
      </div>

    </div>
    <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
        <b>INPUT_TYPE = 'RGB' - LOOP vs SQUEEZE EXPERIMENTS</b>
      </h2>
      <h2 class="subtitle has-text-centered">
        During the course of our experimentation, we conducted several trials to determine the most appropriate method for our model - loop or squeeze. Our findings revealed that the squeeze method was more suitable for our specific model. This conclusion was reached after a thorough analysis of the results obtained from the multiple trials. We firmly believe that this is the optimal choice for achieving the desired outcomes based on our rigorous evaluation.      </h2>
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/5.png" alt="MY ALT TEXT"/>
        </div>

    </div>
    <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
        <b>INPUT_TYPE = 'FLOW'</b>
      </h2>
      <h2 class="subtitle has-text-centered">
        In our study, we evaluated the efficacy of optical flow in analyzing video data. Specifically, we utilized the Lucas Canade algorithm to calculate optical flow between every two consecutive frames in our dataset, which yielded raw flow images. We then computed the magnitude and orientation between these two images and tested the effectiveness of using a combination of flow, magnitude, and orientation data in our analysis.
        To compare the results obtained from these methods, we generated accuracy graphs and compared them with the accuracy of using RGB input modality. This evaluation process was conducted with a rigorous and systematic approach, utilizing multiple trials to ensure the validity of our findings.</h2>
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/6.png" alt="MY ALT TEXT"/>
        </div>

    </div>
    <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
        <b>INPUT_TYPE = 'RGB + FLOW'</b>
      </h2>
      <h2 class="subtitle has-text-centered">
        Lastly, our experimental process involved conducting three distinct trials to compare the efficacy of different input types - RGB, flow, and a combination of RGB and flow - in our CNN-LSTM based model. Our analysis of the results revealed that the RGB input was the most effective modality for our model. Specifically, we observed that the combination of RGB and flow was slightly less effective than the RGB input alone, as illustrated in the plotted data below.
        Our study utilized a rigorous and systematic approach to testing the effectiveness of different input types, employing multiple trials to ensure the validity of our findings. Based on our results, we believe that the RGB input is the optimal choice for achieving the desired outcomes in our CNN-LSTM based model. </h2>
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/7.png" alt="MY ALT TEXT"/>
        </div>

    </div>
    <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
        <b>INPUT_TYPE = 'RGB' - BACKBONE: VGG19 vs RESNET50 EXPERIMENTS</b>
      </h2>
      <h2 class="subtitle has-text-centered">
        In our comprehensive evaluation of the VGG19 and ResNet50 backbones for a specific task, we have found that the VGG19 architecture demonstrates superior performance in terms of accuracy and convergence to 1.00 on the training set. Furthermore, the VGG19 model exhibits a more consistent loss and accuracy plot throughout the training process.
        Although ResNet50 has demonstrated better accuracy in widely recognized image classification tasks such as ImageNet. We think that in our specific task, the dataset used in our evaluation had a simpler structure or contained features that were better captured by the shallower VGG19 network. In such cases, the additional depth of ResNet50 may not provide a significant advantage and could even lead to overfitting or increased complexity without substantial performance gains.</h2>
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/8.png" alt="MY ALT TEXT"/>
        </div>

    </div>
    <div class="item" >
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
        <b>INPUT_TYPE = 'RGB' - FUSION METHOD: EARLY vs LATE EXPERIMENTS</b>
      </h2>
      <h2 class="subtitle has-text-centered">
        For the best configuration, where we use RGB input type, vgg19 as a backbone, numFrames=10 and stride=3, squeeze method and image_size=224; we didn't see any significant change between utilizing early and late fusion methods.</h2>
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/images/9.png" alt="MY ALT TEXT"/>
      </div>

    </div>



  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->








<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<!--   <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->

<!--
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
